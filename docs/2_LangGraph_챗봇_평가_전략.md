# LangGraph 기반 챗봇 평가 전략 요약

본 문서는 LangGraph로 구축된 상태 저장(stateful) 챗봇의 품질을 보증하고 지속적으로 개선하기 위한 구체적이고 현대적인 평가 전략을 요약합니다. 이 전략은 개발의 전체 생명주기에 걸쳐 적용됩니다.

---

### 1. LLM 평가의 새로운 패러다임: 무엇을 측정할 것인가?

단순한 텍스트 일치율(예: ROUGE, BLEU)을 넘어, LLM 챗봇의 품질은 다차원적인 관점에서 측정되어야 합니다.

| 평가 차원             | 주요 내용                                                                                          |
| ------------------- | -------------------------------------------------------------------------------------------------- |
| **정확성 & 충실성**   | 답변이 사실에 부합하는가? 제공된 외부 문서(RAG)에 근거하여 답변하며 환각(Hallucination)은 없는가?     |
| **관련성**            | 답변이 사용자의 질문 의도에 부합하는가? RAG 시스템이 검색한 정보가 질문에 답변하는 데 유용한가?        |
| **일관성 & 가독성**   | 답변이 논리적으로 구성되어 있고, 문법적으로 올바르며, 사용자가 이해하기 쉽게 작성되었는가?               |
| **안전성 & 편향**     | 유해하거나, 공격적이거나, 특정 집단에 편향된 내용을 생성하지 않는가?                                 |
| **작업 완료**         | 단순 질의응답을 넘어, '항공권 예약'과 같이 주어진 작업을 끝까지 성공적으로 완료했는가?                 |

이러한 품질 지표를 측정하기 위해서는 실제 사용 패턴과 도메인 특성을 반영하는 **고품질 평가 데이터셋("골든 데이터셋")**을 구축하는 것이 선행되어야 합니다.

---

### 2. 개발 라이프사이클 단계별 평가 전략 (하이브리드 접근법)

단일 도구가 아닌, 개발 단계별로 최적화된 도구를 조합하여 사용하는 하이브리드 전략이 효과적입니다.

#### **1단계: 개발 및 디버깅**

-   **주요 도구:** **LangSmith**
-   **핵심 활동:**
    -   복잡한 에이전트의 실행 흐름을 단계별로 시각적으로 추적 (Tracing).
    -   상태(State) 객체가 각 노드를 거치며 어떻게 변하는지 확인하여 버그의 근본 원인 파악.
    -   이 단계에서 LangSmith는 평가 도구보다 **필수적인 디버거** 역할을 합니다.

#### **2단계: 자동화된 단위/회귀 테스트 (CI/CD 통합)**

-   **주요 도구:** **DeepEval**, **pytest**
-   **핵심 활동:**
    -   그래프의 개별 노드(함수)를 격리하여 테스트하는 단위 테스트 작성.
    -   LLM API 호출 등 외부 의존성을 모킹(Mocking)하여 빠르고 결정론적인 테스트 환경 구축.
    -   GitHub Actions 등 CI/CD 파이프라인에 통합하여, 코드 변경 시마다 자동으로 핵심 기능의 성능 저하(회귀) 여부를 검증.

#### **3단계: 벤치마킹 및 품질 검증**

-   **주요 도구:** **TruLens**, **RAGAs**
-   **핵심 활동:**
    -   새로운 모델, 프롬프트, 또는 그래프 구조 변경 시, '골든 데이터셋'을 기준으로 성능을 체계적으로 벤치마킹.
    -   **TruLens**: Groundedness(근거 충실성), Relevance(관련성) 등 객관적인 품질 지표를 측정하고 버전 간 성능 비교.
    -   **RAGAs**: RAG 파이프라인의 성능이 저조할 때, 원인이 검색(Retrieval) 문제인지 생성(Generation) 문제인지 심층적으로 진단.

#### **4단계: 프로덕션 모니터링 및 개선**

-   **주요 도구:** **LangSmith**
-   **핵심 활동:**
    -   배포된 챗봇의 비용, 지연 시간, 에러율 등 운영 지표 실시간 추적.
    -   사용자 피드백(예: '좋아요/싫어요')을 수집하고, 예상치 못한 실패 사례 분석.
    -   수집된 실제 상호작용 데이터를 새로운 평가 데이터로 활용하여 **개발-평가의 선순환 구조** 구축.

---

### 3. LangGraph 특화 테스트 고급 기법

LangGraph의 상태 저장 및 순환 구조를 효과적으로 테스트하기 위한 고급 기법입니다.

-   **"스트림 가로채기(Stream Interception)" 패턴:**
    -   `graph.invoke()` 대신 `graph.stream()`을 사용하여, 그래프의 각 노드가 실행될 때마다 전체 상태 객체를 반환받습니다.
    -   테스트 코드에서 챗봇의 실행을 한 단계씩 제어하고, 각 단계의 상태(메시지, 변수 등)가 예상대로 변하는지 검증할 수 있습니다.
    -   이를 통해 복잡한 대화 흐름과 상태 전환 로직을 화이트박스 방식으로 테스트할 수 있습니다.

-   **무한 루프 선제적 테스트:**
    -   무한 루프가 발생할 수 있는 시나리오를 의도적으로 테스트 케이스로 만듭니다.
    -   "스트림 가로채기" 패턴을 활용하여, 루프의 종료 조건이 정상적으로 작동하여 루프를 탈출하는지를 직접 검증합니다. 이는 런타임 에러를 사전에 방지하는 효과적인 전략입니다.

---

### 4. 평가 데이터셋이 없는 경우의 개발 전략: 현실적인 접근법

"골든 데이터셋"이 처음부터 존재하지 않는 전문 분야나 신규 서비스에서는 어떻게 챗봇 개발을 시작하고 고도화할 수 있을까요? 이 경우, 평가의 패러다임을 '정적인 데이터셋'에서 **'동적인 피드백 루프'**로 전환하는 전략이 매우 중요합니다.

#### **1) '사용자'를 살아있는 데이터셋으로 활용하기 (가장 중요)**

가장 현실적이고 강력한 접근법은, 초기 버전의 챗봇을 우선 배포하고 실제 사용자 상호작용을 데이터셋의 원천으로 삼는 것입니다. 이는 문서의 **4단계: 프로덕션 모니터링 및 개선** 전략을 더욱 구체화한 것입니다.

-   **피드백 수집 장치:** 챗봇의 모든 답변에 '좋아요/싫어요'와 같은 간단한 피드백 버튼을 추가하여 사용자 평가를 즉각적으로 수집합니다.
-   **실패 사례 집중 분석:** LangSmith와 같은 추적 도구를 활용해, 사용자가 '싫어요'를 누르거나 대화가 중단된 실패 사례를 집중적으로 분석합니다.
-   **데이터셋으로의 전환:** 이러한 실패 사례와 그에 대한 바람직한 답변을 정리하여, 우리 서비스만의 '골든 데이터셋'을 점진적으로 구축해 나갑니다.

#### **2) LLM을 평가자(Judge)로 활용하기**

정해진 정답(Ground Truth)이 없더라도, 더 뛰어난 상위 모델(예: GPT-4, Claude 3 Opus)을 **평가자(Judge LLM)**로 사용하여 답변의 품질을 정량적으로 측정할 수 있습니다.

-   **작동 원리:** `(질문, 챗봇의 답변)` 쌍을 평가자 LLM에게 전달하고, "이 답변이 질문의 의도에 충실한가?", "답변의 논리는 타당한가?"와 같은 정의된 기준에 따라 점수를 매기게 합니다.
-   **활용 도구:** **TruLens**, **RAGAs**와 같은 최신 평가 프레임워크들은 내부적으로 이런 'LLM-as-a-Judge' 접근법을 사용하여 '관련성', '근거 충실성' 등의 복잡한 지표를 측정합니다.

#### **3) 합성 데이터 생성 (Synthetic Data Generation)**

초기 평가 데이터셋을 빠르게 확보하기 위해, 강력한 LLM에게 역할을 부여하여 가상의 데이터를 생성하게 할 수 있습니다.

-   **예시:** "당신은 OOO 서비스의 전문가입니다. 사용자들이 할 만한 예상 질문과 그에 대한 이상적인 답변 50개를 생성해주세요." 와 같은 프롬프트를 통해 초기 데이터셋을 만듭니다.
-   **장단점:** 개발 초기 단계에서 빠르게 데이터셋을 확보할 수 있다는 장점이 있지만, 실제 사용자 패턴과 다를 수 있고 깊이 있는 엣지 케이스를 다루기에는 한계가 있습니다.

#### **4) 휴리스틱 및 규칙 기반 평가**

완벽한 정답은 없어도, '좋은 답변'이 갖춰야 할 최소한의 요건(휴리스틱)을 정의하고, 이를 통과했는지 자동화된 규칙으로 평가합니다.

-   **예시 (RAG 챗봇):** 답변에 반드시 출처(Source) 링크가 포함되었는지 여부를 체크합니다.
-   **예시 (Tool-using 챗봇):** API 호출에 필요한 모든 인자(parameter)를 사용자 대화에서 빠짐없이 추출했는지 검증합니다.

결론적으로, 데이터셋이 없는 도메인에서는 처음부터 완벽을 추구하기보다, **'배포 → 측정 → 학습'의 선순환 고리**를 얼마나 체계적으로, 그리고 빠르게 구축하느냐가 챗봇의 성패를 좌우합니다.
